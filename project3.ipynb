{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOtBIeyCSRLDapPGYltFmI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fortune901/nubase-project/blob/main/project3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7gIMAJ8OCII"
      },
      "outputs": [],
      "source": [
        "# IMPORTER LES DOCUMENTS DANS LE FICHIER PYTHON\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#importation des bibliothèques nécessaires\n",
        "\n",
        "import pandas as pd # pour lire et manipuler les données tabulaires (fichier.csv)\n",
        "from sklearn.model_selection import train_test_split # pour diviser les données en ensemble d'entrainement et de test\n",
        "from sklearn.preprocessing import StandardScaler # pour normaliser les données\n",
        "from tensorflow.keras.models import Sequential # ici c'est pour construire un modèle de reseau de neurone\n",
        "from tensorflow.keras.layers import Dense # ici c'est pour pouvoir ajouter des couches de neurones cachees\n",
        "\n",
        "import re\n",
        "\n",
        "def convertir_demi_vie_en_secondes(demi_vie_texte):\n",
        "    if not demi_vie_texte or demi_vie_texte.strip() == '':\n",
        "        return None\n",
        "\n",
        "    # Dictionnaire des unités vers secondes\n",
        "    conversion = {\n",
        "        's': 1,\n",
        "        'ms': 1e-3,\n",
        "        'us': 1e-6,\n",
        "        'ns': 1e-9,\n",
        "        'min': 60,\n",
        "        'h': 3600,\n",
        "        'H': 3600,\n",
        "        'd': 86400,\n",
        "        'y': 31536000,\n",
        "        'a': 31536000,  # parfois y = a = année\n",
        "    }\n",
        "\n",
        "    # Extraction avec expression régulière\n",
        "    match = re.match(r'([\\d\\.]+)\\s*([a-zA-Zµ]+)', demi_vie_texte.strip())\n",
        "    if match:\n",
        "        valeur = float(match.group(1))\n",
        "        unite = match.group(2).lower()  # on passe l’unité en minuscule\n",
        "\n",
        "        for cle in conversion:\n",
        "            if cle.lower() == unite:\n",
        "                return valeur * conversion[cle]\n",
        "\n",
        "    return None  # si non reconnu\n",
        "#Extraction des donnees\n",
        "def extraire_donnees_nubase(fichier):\n",
        "  \"\"\"\n",
        "  Extrait les donnees nucleaires depuis le fichier nubase_4.mas20.\n",
        "  \"\"\"\n",
        "  donnees = []\n",
        "  with open(fichier, 'r', encoding='utf-8') as f:\n",
        "    for ligne in f:\n",
        "      if ligne.strip() == '' or ligne.startswith('#'): # cette ligne de code permet de sauter les lignes vides ou de commmentaire\n",
        "        continue\n",
        "      try:\n",
        "        #Lectures des champs depuis les positions fixes (selon la structure de nubase2020)\n",
        "        # attention : les indices peuvent varier selon les versions du fichier\n",
        "        Z = ligne[0:3].strip() # Numero atomique (proton)\n",
        "        N = ligne[4:8].strip() # Nombre de neutrons\n",
        "        A = ligne[9:13].strip() # Nombre de masse\n",
        "        element = ligne[14:17].strip() # Symbole de l'element\n",
        "        Q= ligne[23:37].strip() # état fonfamentale (g) ou isomere (m)\n",
        "        donnees.append({'Z': Z,\n",
        "                       'N': N,\n",
        "                       'A': A,\n",
        "                       'Element': element,\n",
        "                       #'Etat': etat,\n",
        "                       'Q(KeV)': Q\n",
        "        })\n",
        "                        #'Demi-vie (s)': demi_vie_s, #ajout de la demi-vie en secondes\n",
        "                       #'Deformation': deformation,\n",
        "                       #'Shell_effect': shell_effect,\n",
        "                       #'Spin_orbite': spin_orbite})\n",
        "      except IndexError: # si une ligne est trop courte on l'ignore\n",
        "        continue\n",
        "  return donnees\n",
        "\n",
        "# Utilisation du script\n",
        "fichier = 'AME2020.dat'#'nubase_4.mas20'\n",
        "donnees = extraire_donnees_nubase(fichier)\n",
        "#affichage de toutes les lignes extraites\n",
        "for ligne in donnees:\n",
        "  print(ligne)\n"
      ],
      "metadata": {
        "id": "-Jzy5egMJ4c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "# Enregistrement dans un fichier CSV\n",
        "with open('donnees_nubase.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "  champs = ['Z', 'N','A','Element',  'Q(KeV)']\n",
        "  writer = csv.DictWriter(csvfile, fieldnames=champs)\n",
        "  writer.writeheader()\n",
        "  writer.writerows(donnees)\n",
        "\n",
        "# cONVERTIR LA LISTE DE DICTIONNAIRES EN DATAFRAME\n",
        "df = pd.DataFrame(donnees)\n",
        "df\n",
        "df.to_csv(\"donnees_nubase.csv\", index=False)\n",
        "df[['N','Z', 'Q(KeV)']]\n",
        "from google.colab import files\n",
        "files.download(\"donnees_nubase.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "QRjKzkRMc--o",
        "outputId": "dbdff68c-685b-4f7d-a021-39fc645f69ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1746899c-3d91-424b-8621-113932831d7a\", \"donnees_nubase.csv\", 82259)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('donnees_nubase.csv')\n",
        "df.head()\n",
        "#supprimer les colonnes qui ne sont pas utile (optionnel)\n",
        "#df.drop(columns=['Colonneinutile1', 'colonneinutile2'], inplac=True)\n",
        "print(df.dtypes) #Verfier les types de données"
      ],
      "metadata": {
        "id": "rprWVHEpFZvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Supprimer les espaces en trop\n",
        "df.columns = df.columns.str.strip()\n",
        "print(df.isnull().sum()) # regarde les valeurs manquantes"
      ],
      "metadata": {
        "id": "lJAB7sg_RyW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd # Ensure pandas is imported\n",
        "\n",
        "# Cherchons les colonnes numeriques à normaliser\n",
        "scaler = MinMaxScaler()\n",
        "cols_a_normaliser = ['Z', 'N', 'A', 'Q(KeV)']\n",
        "\n",
        "# Convertir les colonnes en type numérique, en gérant les erreurs\n",
        "for col in cols_a_normaliser:\n",
        "    # errors='coerce' remplacera les valeurs non convertibles par NaN\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# Gérer les valeurs manquantes (NaN) créées par la conversion\n",
        "# Option 1: Remplacer les NaNs par une valeur (ex: la moyenne, la médiane, ou 0)\n",
        "# df.fillna(df.mean(), inplace=True) # Remplace par la moyenne de chaque colonne\n",
        "# df.fillna(0, inplace=True)       # Remplace par 0\n",
        "\n",
        "# Option 2: Supprimer les lignes contenant des NaNs dans ces colonnes\n",
        "df.dropna(subset=cols_a_normaliser, inplace=True) # Supprime les lignes si NaN dans l'une des colonnes spécifiées\n",
        "\n",
        "# Maintenant, appliquer le scaler sur les colonnes nettoyées\n",
        "df[cols_a_normaliser] = scaler.fit_transform(df[cols_a_normaliser])\n",
        "\n",
        "print(df.head())\n",
        "print(df.describe())"
      ],
      "metadata": {
        "id": "UllwP3g0KuOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[['Z', 'N', 'A']]  # Ajoute d'autres colonnes si besoin\n",
        "y = df['Q(KeV)']\n",
        "\n",
        "# Exemple de séparation\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42)\n",
        "\n",
        "# choix du modèle de ML\n",
        "#from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "#model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "#model.fit(X_train, y_train)\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "model = MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# validation croisée\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(model, X_train, y_train, cv=7, scoring='neg_mean_squared_error')\n",
        "# evaluation du modèle\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"R²: {r2}\")\n",
        "\n",
        "#prediction de Q\n",
        "Q_pred = model.predict(X_test)\n",
        "print(Q_pred)"
      ],
      "metadata": {
        "id": "15lRk_4aToIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation du modèle de prediction\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, Q_pred))\n",
        "r2 = r2_score(y_test, Q_pred)\n",
        "\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"R²: {r2}\")"
      ],
      "metadata": {
        "id": "dCsunreeVfuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction des demi-vies\n",
        "def predict_half_life(Q_alpha, Z, a, b, c, d):\n",
        "    # Ensure Z and Q_alpha are numpy arrays for element-wise operations\n",
        "    Z = np.asarray(Z)\n",
        "    Q_alpha = np.asarray(Q_alpha)\n",
        "\n",
        "    # Handle potential division by zero or negative Q_alpha values\n",
        "    # Replacing non-positive Q_alpha with a small positive number to avoid errors\n",
        "    Q_alpha[Q_alpha <= 0] = 1e-9 # Replace with a small positive value\n",
        "\n",
        "    log_T = (a * Z + b) / np.sqrt(Q_alpha) + (c * Z + d)\n",
        "    T = 10**log_T  # Convertir log(T) en T\n",
        "    return T\n",
        "\n",
        "# Exemple d'utilisation\n",
        "# Use the 'Z' column from the X_test DataFrame\n",
        "predicted_half_lives = predict_half_life(Q_pred, X_test['Z'], a=-0.5, b=20, c=0.1, d=-5)\n",
        "print(predicted_half_lives)"
      ],
      "metadata": {
        "id": "LNH36mtyV7n1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}